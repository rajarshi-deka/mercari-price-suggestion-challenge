{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECQJvCp2PoPr",
        "colab_type": "text"
      },
      "source": [
        "# Mercari Price Suggestion Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83-icK98Pqb6",
        "colab_type": "text"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fipnuL6H6EH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fxFQKtXZKCb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "3d831d3c-390b-4236-aed8-7e7673e349f6"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfW4BQQfUHAw",
        "colab_type": "text"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_AgmweMUN8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_length(text, no_desc_string):\n",
        "\n",
        "  '''Function to compute the text length only for items with descriptions'''\n",
        "  \n",
        "  try:\n",
        "    if text in no_desc_string:\n",
        "      return 0\n",
        "    else:\n",
        "      return len(text.split())\n",
        "  except:\n",
        "    return 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjuZQjjzr-Fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categ_split(text):\n",
        "  \n",
        "  '''Function to split the category into three parts'''\n",
        "\n",
        "  if len(text.split('/')) == 1:\n",
        "    return 'missing', 'missing', 'missing'\n",
        "  else:    \n",
        "    main_categ, sub_categ_one, sub_categ_two = text.split('/')[:3]\n",
        "    return main_categ, sub_categ_one, sub_categ_two"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsQtGjG5SxA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "brand_set = pickle.load(open(\"brand_set.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzNYSb3DUlkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/valkling/mercari-rnn-2ridge-models-with-notes-0-42755\n",
        "def missing_brand(features):\n",
        "\n",
        "  '''Function to fill the missing brands with words from the name feature'''\n",
        "  \n",
        "  brand = features[0]\n",
        "  name = features[1]\n",
        "  if brand == 'missing':\n",
        "    for word in name.split():\n",
        "      if word in brand_set:        \n",
        "        return word\n",
        "  if name in brand_set:    \n",
        "    return name  \n",
        "  return brand"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA6q07kyY8vK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_stopwords = stopwords.words('english')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYQEF1NYZdba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocess(text, eng_stopwords):\n",
        "\n",
        "  '''Function to perform text preprocessing'''\n",
        "\n",
        "  text = decontracted(text) #Funtion to perform decontractions\n",
        "  text = re.sub(\"[\\-\\\\\\n\\t]\", \" \", text)  #Regex to remove all \\n, \\t, - and \\\n",
        "  text = re.sub(\"[^A-Za-z0-9]\", \" \", text)  #Regex to remove all the words except A-Za-z0-9\n",
        "  text = re.sub('\\s\\s+', ' ', str(text))  #Regex to remove all the extra spaces\n",
        "  text = text.lower() #Converts everything to lower case\n",
        "  text = \" \".join([word for word in text.split() if word not in eng_stopwords]) #Remove stopwords\n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPXbsXHUZi5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stackoverflow.com/a/47091490/4084039\n",
        "def decontracted(phrase):\n",
        "\n",
        "    '''Function to perform decontraction'''\n",
        "    \n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKx0B4AuPuzE",
        "colab_type": "text"
      },
      "source": [
        "## Function 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJs0yw1RPztF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_fun_1(datapoint):\n",
        "\n",
        "  # **********Text Preprocessing**********\n",
        "  \n",
        "  no_desc_string = 'No description yet'  \n",
        "  datapoint['item_desc_length'] = datapoint['item_description'].apply(lambda x : text_length(x, no_desc_string))\n",
        "  datapoint['name_length'] = datapoint['name'].apply(lambda x : text_length(x, no_desc_string))\n",
        "  datapoint['category_name'].fillna(value='missing', inplace=True)\n",
        "  datapoint['main_categ'], datapoint['sub_categ_one'], datapoint['sub_categ_two'] = zip(\n",
        "      *datapoint['category_name'].apply(lambda x : categ_split(x)))\n",
        "  datapoint['brand_name'].fillna(value='missing', inplace=True)  \n",
        "  datapoint['brand'] = datapoint[['brand_name', 'name']].apply(missing_brand, axis=1)\n",
        "  datapoint['item_description'].fillna(value='missing', inplace=True)\n",
        "  datapoint['item_description'] = datapoint['item_description'].replace(no_desc_string, 'missing')\n",
        "  datapoint['item_desc_preprocess'] = datapoint['item_description'].apply(lambda x : text_preprocess(x, eng_stopwords))\n",
        "  datapoint['name_preprocess'] = datapoint['name'].apply(lambda x : text_preprocess(x, eng_stopwords))\n",
        "  datapoint.drop(columns=['train_id', 'name', 'category_name', 'brand_name', 'price', 'item_description'], axis=1, inplace=True)\n",
        "\n",
        "  # **********Feature Transformation**********  \n",
        "  \n",
        "  item_cond_vectorizer, ship_vectorizer, main_categ_vectorizer, sub_categ_one_vectorizer, sub_categ_two_vectorizer, \n",
        "  brand_vectorizer, item_desc_length_minmax_scaler, name_length_minmax_scaler, name_preprocess_vectorizer, \n",
        "  item_desc_preprocess_vectorizer = pickle.load(open(\"train_fit_transform.pkl\", 'rb'))\n",
        "  \n",
        "  item_condition = item_cond_vectorizer.transform(datapoint['item_condition_id'].values.reshape(-1,1))  \n",
        "  shipping = ship_vectorizer.transform(datapoint['shipping'].values.reshape(-1,1))  \n",
        "  main_categ = main_categ_vectorizer.transform(datapoint['main_categ'].astype(str))  \n",
        "  sub_categ_one = sub_categ_one_vectorizer.transform(datapoint['sub_categ_one'].astype(str))  \n",
        "  sub_categ_two = sub_categ_two_vectorizer.transform(datapoint['sub_categ_two'].astype(str))  \n",
        "  brand = brand_vectorizer.transform(datapoint['brand'].astype(str))  \n",
        "  item_desc_length = item_desc_length_minmax_scaler.transform(datapoint['item_desc_length'].values.reshape(-1,1))  \n",
        "  name_length = name_length_minmax_scaler.transform(datapoint['name_length'].values.reshape(-1,1))  \n",
        "  name = name_preprocess_vectorizer.transform(datapoint['name_preprocess'].values)  \n",
        "  item_desc_tfidf = item_desc_preprocess_vectorizer.transform(datapoint['item_desc_preprocess'].values)\n",
        "\n",
        "  # **********Feature Merging**********\n",
        "\n",
        "  feature_merged_tfidf = hstack((item_condition, shipping, main_categ, sub_categ_one, sub_categ_two, \n",
        "                         brand, item_desc_length, name_length, name, \n",
        "                         item_desc_tfidf)).tocsr()\n",
        "\n",
        "  # **********Model Prediction**********\n",
        "\n",
        "  lgbm_model = pickle.load(open(\"lgbm_model_tfidf_modif_0_4320.pkl\", 'rb'))\n",
        "  y_pred = lgbm_model.predict(feature_merged_tfidf)\n",
        "  \n",
        "  return y_pred"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtfzpetjZIza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('train.tsv', delimiter='\\t')\n",
        "datapoint = df.loc[[0]]\n",
        "y_pred = final_fun_1(datapoint)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNMMGvuhyaOz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6bab67c-6669-4817-dafd-d9c145e481ed"
      },
      "source": [
        "print(\"y_pred: \", y_pred)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_pred:  [2.37330328]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPFwdzsmdyRz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "51278099-b84b-43c4-bff2-f551961a5cd8"
      },
      "source": [
        "print(\"y_target: \", df.loc[0][5])\n",
        "print(\"y_pred: \", np.exp(2.37330328))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_target:  10.0\n",
            "y_pred:  10.732787193558792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RmMygO01ARC",
        "colab_type": "text"
      },
      "source": [
        "## Function 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl-zkB9p0sXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_fun_2(datapoint, y_target):\n",
        "\n",
        "  # **********Text Preprocessing**********\n",
        "  \n",
        "  no_desc_string = 'No description yet'  \n",
        "  datapoint['item_desc_length'] = datapoint['item_description'].apply(lambda x : text_length(x, no_desc_string))\n",
        "  datapoint['name_length'] = datapoint['name'].apply(lambda x : text_length(x, no_desc_string))\n",
        "  datapoint['category_name'].fillna(value='missing', inplace=True)\n",
        "  datapoint['main_categ'], datapoint['sub_categ_one'], datapoint['sub_categ_two'] = zip(\n",
        "      *datapoint['category_name'].apply(lambda x : categ_split(x)))\n",
        "  datapoint['brand_name'].fillna(value='missing', inplace=True)\n",
        "  datapoint['brand'] = datapoint[['brand_name', 'name']].apply(missing_brand, axis=1)\n",
        "  datapoint['item_description'].fillna(value='missing', inplace=True)\n",
        "  datapoint['item_description'] = datapoint['item_description'].replace(no_desc_string, 'missing')\n",
        "  datapoint['item_desc_preprocess'] = datapoint['item_description'].apply(lambda x : text_preprocess(x, eng_stopwords))\n",
        "  datapoint['name_preprocess'] = datapoint['name'].apply(lambda x : text_preprocess(x, eng_stopwords))\n",
        "  datapoint.drop(columns=['train_id', 'name', 'category_name', 'brand_name', 'price', 'item_description'], axis=1, inplace=True)\n",
        "\n",
        "  # **********Feature Transformation**********  \n",
        "\n",
        "  item_cond_vectorizer, ship_vectorizer, main_categ_vectorizer, sub_categ_one_vectorizer, sub_categ_two_vectorizer, \n",
        "  brand_vectorizer, item_desc_length_minmax_scaler, name_length_minmax_scaler, name_preprocess_vectorizer, \n",
        "  item_desc_preprocess_vectorizer = pickle.load(open(\"train_fit_transform.pkl\", 'rb'))\n",
        "  \n",
        "  item_condition = item_cond_vectorizer.transform(datapoint['item_condition_id'].values.reshape(-1,1))  \n",
        "  shipping = ship_vectorizer.transform(datapoint['shipping'].values.reshape(-1,1))  \n",
        "  main_categ = main_categ_vectorizer.transform(datapoint['main_categ'].astype(str))  \n",
        "  sub_categ_one = sub_categ_one_vectorizer.transform(datapoint['sub_categ_one'].astype(str))  \n",
        "  sub_categ_two = sub_categ_two_vectorizer.transform(datapoint['sub_categ_two'].astype(str))  \n",
        "  brand = brand_vectorizer.transform(datapoint['brand'].astype(str))  \n",
        "  item_desc_length = item_desc_length_minmax_scaler.transform(datapoint['item_desc_length'].values.reshape(-1,1))  \n",
        "  name_length = name_length_minmax_scaler.transform(datapoint['name_length'].values.reshape(-1,1))  \n",
        "  name = name_preprocess_vectorizer.transform(datapoint['name_preprocess'].values)  \n",
        "  item_desc_tfidf = item_desc_preprocess_vectorizer.transform(datapoint['item_desc_preprocess'].values)\n",
        "\n",
        "  # **********Feature Merging**********\n",
        "\n",
        "  feature_merged_tfidf = hstack((item_condition, shipping, main_categ, sub_categ_one, sub_categ_two, \n",
        "                         brand, item_desc_length, name_length, name, \n",
        "                         item_desc_tfidf)).tocsr()\n",
        "\n",
        "  # **********Model Prediction**********\n",
        "\n",
        "  lgbm_model = pickle.load(open(\"lgbm_model_tfidf_modif_0_4320.pkl\", 'rb'))\n",
        "  y_pred = lgbm_model.predict(feature_merged_tfidf)\n",
        "  y_pred = y_pred[0]  \n",
        "\n",
        "  # **********RMSLE**********\n",
        "  \n",
        "  rmsle = np.sqrt(mean_squared_error([y_target], [y_pred]))\n",
        "\n",
        "  return rmsle"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX8qWnP91mpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapoint = df.loc[[0]]\n",
        "y_target = np.float(np.log(datapoint['price'].values+1))\n",
        "rmsle = final_fun_2(datapoint, y_target)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkyxZHTe1qsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "905a586c-2c91-4501-8807-848b003ca8e8"
      },
      "source": [
        "print(\"RMSLE: \", rmsle)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSLE:  0.024591988306498003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHGLp_O01_wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}